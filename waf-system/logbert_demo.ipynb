{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442ee734",
   "metadata": {},
   "source": [
    "# LogBERT-style Transformer WAF: Benign-only Training and Malicious Payload Evaluation\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Parsing HTTP access logs into sequences.\n",
    "- Training a LogBERT-style transformer on benign traffic only.\n",
    "- Scoring custom payloads and computing simple metrics vs. benign samples.\n",
    "\n",
    "Prerequisites: run your virtualenv and ensure `waf-system/requirements.txt` is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e24e5aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T01:43:45.247173Z",
     "iopub.status.busy": "2025-09-24T01:43:45.246859Z",
     "iopub.status.idle": "2025-09-24T01:43:47.812657Z",
     "shell.execute_reply": "2025-09-24T01:43:47.812414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAF_ROOT = /Users/majjipradeepkumar/Downloads/WAF/Sample-apps-for-training-a-transformer-based-WAF-pipleline/waf-system\n"
     ]
    }
   ],
   "source": [
    "# Imports and workspace setup\n",
    "from pathlib import Path\n",
    "import sys, os, time, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Torch / training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Find waf-system root (search upwards for the folder)\n",
    "CWD = Path.cwd().resolve()\n",
    "WAF_ROOT = None\n",
    "for p in [CWD] + list(CWD.parents):\n",
    "    if (p / 'waf-system').is_dir():\n",
    "        WAF_ROOT = p / 'waf-system'\n",
    "        break\n",
    "if WAF_ROOT is None:\n",
    "    # If this notebook is already inside waf-system, use it\n",
    "    if (CWD / 'ml-pipeline').is_dir():\n",
    "        WAF_ROOT = CWD\n",
    "    else:\n",
    "        raise RuntimeError('Could not locate waf-system root from current working directory')\n",
    "\n",
    "# Add pipeline paths for imports\n",
    "sys.path.insert(0, str(WAF_ROOT / 'ml-pipeline'))\n",
    "sys.path.insert(0, str(WAF_ROOT / 'ml-pipeline' / 'preprocessing'))\n",
    "sys.path.insert(0, str(WAF_ROOT / 'ml-pipeline' / 'training'))\n",
    "\n",
    "from log_processor import LogPreprocessor\n",
    "from trainer import WAFTrainer, prepare_training_data, collate_fn\n",
    "from waf_model import create_waf_model\n",
    "\n",
    "print('WAF_ROOT =', WAF_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40dafb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T01:43:47.813838Z",
     "iopub.status.busy": "2025-09-24T01:43:47.813667Z",
     "iopub.status.idle": "2025-09-24T01:43:47.890558Z",
     "shell.execute_reply": "2025-09-24T01:43:47.890336Z"
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Non-relative patterns are unsupported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m parsed_cache = []\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m log_globs:\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Expand glob manually\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     paths = \u001b[38;5;28mlist\u001b[39m(\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ch \u001b[38;5;129;01min\u001b[39;00m pattern \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m*?[]\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [Path(pattern)])\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.is_file():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py:589\u001b[39m, in \u001b[36mPath.glob\u001b[39m\u001b[34m(self, pattern, case_sensitive, recurse_symlinks)\u001b[39m\n\u001b[32m    587\u001b[39m     pattern = \u001b[38;5;28mself\u001b[39m.with_segments(pattern)\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pattern.anchor:\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNon-relative patterns are unsupported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    590\u001b[39m parts = pattern._tail.copy()\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parts:\n",
      "\u001b[31mNotImplementedError\u001b[39m: Non-relative patterns are unsupported"
     ]
    }
   ],
   "source": [
    "# Load benign access logs and build token sequences\n",
    "from datetime import datetime\n",
    "\n",
    "preprocessor = LogPreprocessor()\n",
    "\n",
    "# Default log locations (nginx and tomcat)\n",
    "log_globs = [\n",
    "    str(WAF_ROOT / 'data' / 'logs' / 'access.log'),\n",
    "    str(WAF_ROOT / 'tomcat' / 'current' / 'logs' / 'localhost_access_log*.txt'),\n",
    "]\n",
    "\n",
    "max_lines = 5000\n",
    "sequences = []\n",
    "parsed_cache = []\n",
    "\n",
    "for pattern in log_globs:\n",
    "    # Expand glob manually\n",
    "    paths = list(Path().glob(pattern) if any(ch in pattern for ch in '*?[]') else [Path(pattern)])\n",
    "    for path in paths:\n",
    "        if not path.exists() or not path.is_file():\n",
    "            continue\n",
    "        with path.open('r', errors='ignore') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if len(sequences) >= max_lines:\n",
    "                    break\n",
    "                pl = line.strip()\n",
    "                if not pl:\n",
    "                    continue\n",
    "                processed = preprocessor.process_log_entry(pl)\n",
    "                if not processed:\n",
    "                    continue\n",
    "                parsed = processed['parsed']\n",
    "                # Build minimal sequence: METHOD, PATH, STATUS, Agent\n",
    "                seq = [\n",
    "                    parsed.get('method', 'GET'),\n",
    "                    parsed.get('path_only', '/'),\n",
    "                    str(parsed.get('status', 200)),\n",
    "                ]\n",
    "                ua = parsed.get('http_user_agent', '') or ''\n",
    "                if 'Mozilla' in ua:\n",
    "                    seq.append('Mozilla')\n",
    "                elif 'curl' in ua:\n",
    "                    seq.append('curl')\n",
    "                elif 'python' in ua.lower():\n",
    "                    seq.append('python')\n",
    "                else:\n",
    "                    seq.append('Other-Agent')\n",
    "                sequences.append(seq)\n",
    "                parsed_cache.append(parsed)\n",
    "        if len(sequences) >= max_lines:\n",
    "            break\n",
    "\n",
    "if len(sequences) < 50:\n",
    "    # Fallback synthetic benign sequences if logs are scarce\n",
    "    base = [\n",
    "        ['GET', '/ecommerce/', '200', 'Mozilla'],\n",
    "        ['GET', '/ecommerce/products', '200', 'Mozilla'],\n",
    "        ['GET', '/blog-cms/', '200', 'Mozilla'],\n",
    "        ['GET', '/rest-api/api/users', '200', 'curl']\n",
    "    ]\n",
    "    sequences.extend(base * 50)\n",
    "\n",
    "print(f\"Collected {len(sequences)} benign sequences for training\")\n",
    "print('Sample:', sequences[0] if sequences else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16997bf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T01:43:47.891546Z",
     "iopub.status.busy": "2025-09-24T01:43:47.891471Z",
     "iopub.status.idle": "2025-09-24T01:43:48.016562Z",
     "shell.execute_reply": "2025-09-24T01:43:48.016321Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/majjipradeepkumar/Downloads/WAF/Sample-apps-for-training-a-transformer-based-WAF-pipleline/waf-system/python-env/lib/python3.13/site-packages/torch/utils/data/dataset.py:469: UserWarning: Length of split at index 0 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m train_ds, val_ds = prepare_training_data(sequences, tokenizer)\n\u001b[32m      5\u001b[39m batch_size = \u001b[32m32\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m train_loader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn=collate_fn)\n\u001b[32m      9\u001b[39m device = \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/WAF/Sample-apps-for-training-a-transformer-based-WAF-pipleline/waf-system/python-env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:388\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    390\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/WAF/Sample-apps-for-training-a-transformer-based-WAF-pipleline/waf-system/python-env/lib/python3.13/site-packages/torch/utils/data/sampler.py:156\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# Create model, tokenize, and prepare datasets\n",
    "model, tokenizer = create_waf_model()\n",
    "train_ds, val_ds = prepare_training_data(sequences, tokenizer)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trainer = WAFTrainer(model, tokenizer, device=device)\n",
    "\n",
    "print('Train size:', len(train_ds), 'Val size:', len(val_ds), 'Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6876a62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T01:43:48.017495Z",
     "iopub.status.busy": "2025-09-24T01:43:48.017416Z",
     "iopub.status.idle": "2025-09-24T01:43:48.031242Z",
     "shell.execute_reply": "2025-09-24T01:43:48.030999Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train briefly (1 epoch) and evaluate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_metrics = \u001b[43mtrainer\u001b[49m.train_epoch(train_loader)\n\u001b[32m      3\u001b[39m val_metrics = trainer.evaluate(val_loader)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTrain metrics:\u001b[39m\u001b[33m'\u001b[39m, train_metrics)\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Train briefly (1 epoch) and evaluate\n",
    "train_metrics = trainer.train_epoch(train_loader)\n",
    "val_metrics = trainer.evaluate(val_loader)\n",
    "\n",
    "print('Train metrics:', train_metrics)\n",
    "print('Val metrics:', val_metrics)\n",
    "\n",
    "# Save a copy for reuse\n",
    "models_dir = WAF_ROOT / 'data' / 'models'\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "save_path = models_dir / 'notebook_model.pt'\n",
    "trainer.save_model(str(save_path))\n",
    "print('Model saved to', save_path)\n",
    "\n",
    "# Switch to eval\n",
    "trainer.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb2d4b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T01:43:48.032243Z",
     "iopub.status.busy": "2025-09-24T01:43:48.032169Z",
     "iopub.status.idle": "2025-09-24T01:43:48.058254Z",
     "shell.execute_reply": "2025-09-24T01:43:48.057994Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vx/y9kq3zf57t14pxj6sfyg0wf00000gn/T/ipykernel_63905/705801105.py:25: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().strftime('%d/%b/%Y:%H:%M:%S +0000')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m     is_anom = score > threshold\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m score, is_anom\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mscore_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/ecommerce/health\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/WAF/Sample-apps-for-training-a-transformer-based-WAF-pipleline/waf-system/python-env/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mscore_request\u001b[39m\u001b[34m(method, uri, user_agent, threshold)\u001b[39m\n\u001b[32m     36\u001b[39m input_ids = enc[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     37\u001b[39m attention_mask = enc[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m outputs = \u001b[43mtrainer\u001b[49m.model(input_ids=input_ids, attention_mask=attention_mask)\n\u001b[32m     39\u001b[39m score = \u001b[38;5;28mfloat\u001b[39m(outputs[\u001b[33m'\u001b[39m\u001b[33manomaly_score\u001b[39m\u001b[33m'\u001b[39m].item())\n\u001b[32m     40\u001b[39m is_anom = score > threshold\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Helper functions for scoring arbitrary URIs/payloads\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "def build_sequence_from_parsed(parsed: Dict[str, Any]) -> List[str]:\n",
    "    seq = [\n",
    "        parsed.get('method', 'GET'),\n",
    "        parsed.get('path_only', '/'),\n",
    "        str(parsed.get('status', 200)),\n",
    "    ]\n",
    "    ua = parsed.get('http_user_agent', '') or ''\n",
    "    if 'Mozilla' in ua:\n",
    "        seq.append('Mozilla')\n",
    "    elif 'curl' in ua:\n",
    "        seq.append('curl')\n",
    "    elif 'python' in ua.lower():\n",
    "        seq.append('python')\n",
    "    else:\n",
    "        seq.append('Other-Agent')\n",
    "    return seq\n",
    "\n",
    "# Construct a common log-format line for a test request\n",
    "from datetime import datetime\n",
    "\n",
    "def make_log_line(method: str, uri: str, user_agent: str = 'Mozilla/5.0', ip: str = '127.0.0.1') -> str:\n",
    "    ts = datetime.utcnow().strftime('%d/%b/%Y:%H:%M:%S +0000')\n",
    "    return f'{ip} - - [{ts}] \"{method} {uri} HTTP/1.1\" 200 0 \"-\" \"{user_agent}\"'\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_request(method: str, uri: str, user_agent: str = 'Mozilla/5.0', threshold: float = 0.5) -> Tuple[float, bool]:\n",
    "    line = make_log_line(method, uri, user_agent)\n",
    "    processed = preprocessor.process_log_entry(line)\n",
    "    if not processed:\n",
    "        return 0.0, False\n",
    "    seq = build_sequence_from_parsed(processed['parsed'])\n",
    "    enc = tokenizer.encode(seq, max_length=128)\n",
    "    input_ids = enc['input_ids'].unsqueeze(0)\n",
    "    attention_mask = enc['attention_mask'].unsqueeze(0)\n",
    "    outputs = trainer.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    score = float(outputs['anomaly_score'].item())\n",
    "    is_anom = score > threshold\n",
    "    return score, is_anom\n",
    "\n",
    "print(score_request('GET', '/ecommerce/health'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f340d239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T01:43:48.059110Z",
     "iopub.status.busy": "2025-09-24T01:43:48.059051Z",
     "iopub.status.idle": "2025-09-24T01:43:48.080071Z",
     "shell.execute_reply": "2025-09-24T01:43:48.079812Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vx/y9kq3zf57t14pxj6sfyg0wf00000gn/T/ipykernel_63905/705801105.py:25: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().strftime('%d/%b/%Y:%H:%M:%S +0000')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m rows = []\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m uri \u001b[38;5;129;01min\u001b[39;00m random.sample(benign_uris, \u001b[38;5;28mmin\u001b[39m(\u001b[32m50\u001b[39m, \u001b[38;5;28mlen\u001b[39m(benign_uris))):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     s, yhat = \u001b[43mscore_request\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     rows.append({\n\u001b[32m     27\u001b[39m         \u001b[33m'\u001b[39m\u001b[33muri\u001b[39m\u001b[33m'\u001b[39m: uri,\n\u001b[32m     28\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m     29\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m: s,\n\u001b[32m     30\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mpred\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(yhat)\n\u001b[32m     31\u001b[39m     })\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m uri \u001b[38;5;129;01min\u001b[39;00m malicious_payloads:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/WAF/Sample-apps-for-training-a-transformer-based-WAF-pipleline/waf-system/python-env/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mscore_request\u001b[39m\u001b[34m(method, uri, user_agent, threshold)\u001b[39m\n\u001b[32m     36\u001b[39m input_ids = enc[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     37\u001b[39m attention_mask = enc[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m outputs = \u001b[43mtrainer\u001b[49m.model(input_ids=input_ids, attention_mask=attention_mask)\n\u001b[32m     39\u001b[39m score = \u001b[38;5;28mfloat\u001b[39m(outputs[\u001b[33m'\u001b[39m\u001b[33manomaly_score\u001b[39m\u001b[33m'\u001b[39m].item())\n\u001b[32m     40\u001b[39m is_anom = score > threshold\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Build a small eval set: benign vs malicious payloads\n",
    "random.seed(42)\n",
    "\n",
    "# Benign URIs from parsed_cache or defaults\n",
    "benign_uris = []\n",
    "for p in parsed_cache:\n",
    "    benign_uris.append(p.get('path_only', '/'))\n",
    "    if len(benign_uris) >= 50:\n",
    "        break\n",
    "if not benign_uris:\n",
    "    benign_uris = ['/ecommerce/', '/ecommerce/products', '/blog-cms/', '/rest-api/api/users'] * 10\n",
    "\n",
    "malicious_payloads = [\n",
    "    \"/ecommerce/search?q=' OR '1'='1\",\n",
    "    \"/ecommerce/search?q=%27%20OR%201%3D1--\",\n",
    "    \"/ecommerce/product?id=1;DROP TABLE users;--\",\n",
    "    \"/blog-cms/?q=<script>alert(1)</script>\",\n",
    "    \"/rest-api/api/users/../../../../etc/passwd\",\n",
    "    \"/rest-api/api/users?name=`cat /etc/passwd`\",\n",
    "]\n",
    "\n",
    "# Score samples\n",
    "rows = []\n",
    "for uri in random.sample(benign_uris, min(50, len(benign_uris))):\n",
    "    s, yhat = score_request('GET', uri)\n",
    "    rows.append({\n",
    "        'uri': uri,\n",
    "        'label': 0,\n",
    "        'score': s,\n",
    "        'pred': int(yhat)\n",
    "    })\n",
    "for uri in malicious_payloads:\n",
    "    s, yhat = score_request('GET', uri)\n",
    "    rows.append({\n",
    "        'uri': uri,\n",
    "        'label': 1,\n",
    "        'score': s,\n",
    "        'pred': int(yhat)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d463c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T01:43:48.081006Z",
     "iopub.status.busy": "2025-09-24T01:43:48.080931Z",
     "iopub.status.idle": "2025-09-24T01:43:48.091090Z",
     "shell.execute_reply": "2025-09-24T01:43:48.090883Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compute simple metrics\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m precision_recall_fscore_support, accuracy_score, roc_auc_score\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y_true = \u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m      5\u001b[39m y_pred = df[\u001b[33m'\u001b[39m\u001b[33mpred\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m      6\u001b[39m y_score = df[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m].values\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute simple metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "\n",
    "y_true = df['label'].values\n",
    "y_pred = df['pred'].values\n",
    "y_score = df['score'].values\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "except Exception:\n",
    "    auc = float('nan')\n",
    "\n",
    "print({'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'roc_auc': auc})\n",
    "\n",
    "# Show top suspicious by score\n",
    "df.sort_values('score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd8202",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- This demo uses only benign traffic for training, so performance against malicious payloads depends on how \"out-of-distribution\" they appear.\n",
    "- Improve results by adding more benign coverage, tuning thresholding, and expanding the sequence/features (templates, params, headers).\n",
    "- For continuous learning, periodically retrain with fresh benign logs and validate on a held-out malicious set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
